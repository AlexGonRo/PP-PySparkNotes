{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da3c717-1f4a-41eb-8e5d-97b327fc8a7c",
   "metadata": {},
   "source": [
    "# PYSPARK PRACTICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f762d-88d2-46eb-9daf-a14b10007649",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dea9188-ab81-4a98-9f94-c517b781e146",
   "metadata": {},
   "source": [
    "I'll be working with the following *dataset*: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceea45b7-33bb-47f4-bf87-8c30512b1ddc",
   "metadata": {},
   "source": [
    "We'll be working with high-level operations, so we need to create a SparkSession (a unified entrypoint for Spark that contains SparkContext, StreamingContext and SQLContext). If we wanted to work directly with RDDs, using a SparkContext would be more appropiate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd9e20-cc4b-479f-b436-c6a1b9345348",
   "metadata": {},
   "source": [
    "// TODO I'd like to test SparkContext at some point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc040af5-71d4-4565-a41e-d5f99273008d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/14 20:19:39 WARN Utils: Your hostname, alejandro resolves to a loopback address: 127.0.1.1; using 192.168.3.37 instead (on interface wlp2s0)\n",
      "25/04/14 20:19:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/14 20:19:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.3.37:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Demo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7b998bf0b440>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "   SparkSession.builder.appName('Spark Demo') \n",
    "  .master('local[*]') #local[*] - Run Spark locally with as many worker threads as logical cores on your machine. I could choose any number really\n",
    "                    # YARN when deploying to a cluster with YARN\n",
    "  .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"100\")  # For testing purposes\n",
    "  .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff888385-053d-49fe-854c-2494236346ca",
   "metadata": {},
   "source": [
    "I won´t be using it here, but I should make an honorary mention to spark-submit, the script used to deploy jobs in a Spark cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df4e4a-e344-47c9-91b9-0eb8b072380a",
   "metadata": {},
   "source": [
    "./bin/spark-submit \\\n",
    "  --class <main-class> \\\n",
    "  --master <master-url> \\\n",
    "  --deploy-mode <deploy-mode> \\\n",
    "  --conf <key>=<value> \\\n",
    "  --py-files file1.py,file2.py,file3.zip,file4.egg \\ # These will be passed to the worker nodes\n",
    "  --archives conda_env.tag.gz \\ # We could package the whole conda environment if we wanted to. Or a virtualEnv \n",
    "  ... # other options\n",
    "  <application-jar> \\\n",
    "  [application-arguments]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ddd712-b94e-447b-a143-7c243c5aeb38",
   "metadata": {},
   "source": [
    "Before we continue, some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "095cbd0c-f400-4c8d-a373-a328ba243579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c24357-dce0-460b-a6c4-db4c02590914",
   "metadata": {},
   "source": [
    "## Explore the data - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e970084-8d38-41a3-9732-953557b2f167",
   "metadata": {},
   "source": [
    "Let's just load a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "671c38c0-71b2-40d3-bf26-c41e7cff070c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=============================>                             (4 + 4) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Time: double (nullable = true)\n",
      " |-- V1: double (nullable = true)\n",
      " |-- V2: double (nullable = true)\n",
      " |-- V3: double (nullable = true)\n",
      " |-- V4: double (nullable = true)\n",
      " |-- V5: double (nullable = true)\n",
      " |-- V6: double (nullable = true)\n",
      " |-- V7: double (nullable = true)\n",
      " |-- V8: double (nullable = true)\n",
      " |-- V9: double (nullable = true)\n",
      " |-- V10: double (nullable = true)\n",
      " |-- V11: double (nullable = true)\n",
      " |-- V12: double (nullable = true)\n",
      " |-- V13: double (nullable = true)\n",
      " |-- V14: double (nullable = true)\n",
      " |-- V15: double (nullable = true)\n",
      " |-- V16: double (nullable = true)\n",
      " |-- V17: double (nullable = true)\n",
      " |-- V18: double (nullable = true)\n",
      " |-- V19: double (nullable = true)\n",
      " |-- V20: double (nullable = true)\n",
      " |-- V21: double (nullable = true)\n",
      " |-- V22: double (nullable = true)\n",
      " |-- V23: double (nullable = true)\n",
      " |-- V24: double (nullable = true)\n",
      " |-- V25: double (nullable = true)\n",
      " |-- V26: double (nullable = true)\n",
      " |-- V27: double (nullable = true)\n",
      " |-- V28: double (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "data_path = './spark_files_master/creditcard.csv'\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True) # It is reading the file from my local file system, not the master or worker nodes\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f544efb8-f09e-4822-b53f-5310f866920d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/14 20:20:04 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+-------------------+----------------+----------------+------------------+-----------------+-----------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+----------------+------------------+-----------------+------------------+-----------------+-----------------+------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+-------------------+------+-----+\n",
      "|Time|              V1|                 V2|              V3|              V4|                V5|               V6|               V7|                V8|               V9|               V10|               V11|               V12|               V13|               V14|             V15|               V16|              V17|               V18|              V19|              V20|               V21|              V22|               V23|               V24|              V25|               V26|              V27|                V28|Amount|Class|\n",
      "+----+----------------+-------------------+----------------+----------------+------------------+-----------------+-----------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+----------------+------------------+-----------------+------------------+-----------------+-----------------+------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+-------------------+------+-----+\n",
      "| 0.0|-1.3598071336738|-0.0727811733098497|2.53634673796914|1.37815522427443|-0.338320769942518|0.462387777762292|0.239598554061257|0.0986979012610507|0.363786969611213|0.0907941719789316|-0.551599533260813|-0.617800855762348|-0.991389847235408|-0.311169353699879|1.46817697209427|-0.470400525259478|0.207971241929242|0.0257905801985591|0.403992960255733|0.251412098239705|-0.018306777944153|0.277837575558899|-0.110473910188767|0.0669280749146731|0.128539358273528|-0.189114843888824|0.133558376740387|-0.0210530534538215|149.62|    0|\n",
      "+----+----------------+-------------------+----------------+----------------+------------------+-----------------+-----------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+----------------+------------------+-----------------+------------------+-----------------+-----------------+------------------+-----------------+------------------+------------------+-----------------+------------------+-----------------+-------------------+------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d46fb5-85bd-422f-9a3d-d6decb29d187",
   "metadata": {},
   "source": [
    "Three important terms to remember here:\n",
    "* *StructType*: A built-in DataType from org.apache.spark.sql.types that implements scala.collection.Seq<StructField>. Basically, it is a `Seq` of `StructField`. \n",
    "* *StructField*: The name, type and default `null` value of a row. You could see it as metadata of whatever you'll find in a row.\n",
    "* *Row*: The values of the \"column\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fbf838-0857-456e-870d-ec530bb97469",
   "metadata": {},
   "source": [
    "Let's do something a little bit more complicated. Let us show columns in groups of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76353eb3-e4ee-45b3-9d56-d29e3c26b080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+-------------------+----------------+\n",
      "|Time|              V1|                 V2|              V3|\n",
      "+----+----------------+-------------------+----------------+\n",
      "| 0.0|-1.3598071336738|-0.0727811733098497|2.53634673796914|\n",
      "| 0.0|1.19185711131486|   0.26615071205963|0.16648011335321|\n",
      "+----+----------------+-------------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+------------------+-------------------+-------------------+\n",
      "|               V4|                V5|                 V6|                 V7|\n",
      "+-----------------+------------------+-------------------+-------------------+\n",
      "| 1.37815522427443|-0.338320769942518|  0.462387777762292|  0.239598554061257|\n",
      "|0.448154078460911|0.0600176492822243|-0.0823608088155687|-0.0788029833323113|\n",
      "+-----------------+------------------+-------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+------------------+------------------+------------------+------------------+\n",
      "|                V8|                V9|               V10|               V11|\n",
      "+------------------+------------------+------------------+------------------+\n",
      "|0.0986979012610507| 0.363786969611213|0.0907941719789316|-0.551599533260813|\n",
      "|0.0851016549148104|-0.255425128109186|-0.166974414004614|  1.61272666105479|\n",
      "+------------------+------------------+------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+------------------+------------------+------------------+-----------------+\n",
      "|               V12|               V13|               V14|              V15|\n",
      "+------------------+------------------+------------------+-----------------+\n",
      "|-0.617800855762348|-0.991389847235408|-0.311169353699879| 1.46817697209427|\n",
      "|  1.06523531137287|  0.48909501589608|-0.143772296441519|0.635558093258208|\n",
      "+------------------+------------------+------------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+------------------+------------------+------------------+------------------+\n",
      "|               V16|               V17|               V18|               V19|\n",
      "+------------------+------------------+------------------+------------------+\n",
      "|-0.470400525259478| 0.207971241929242|0.0257905801985591| 0.403992960255733|\n",
      "| 0.463917041022171|-0.114804663102346|-0.183361270123994|-0.145783041325259|\n",
      "+------------------+------------------+------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------------+------------------+------------------+------------------+\n",
      "|                V20|               V21|               V22|               V23|\n",
      "+-------------------+------------------+------------------+------------------+\n",
      "|  0.251412098239705|-0.018306777944153| 0.277837575558899|-0.110473910188767|\n",
      "|-0.0690831352230203|-0.225775248033138|-0.638671952771851| 0.101288021253234|\n",
      "+-------------------+------------------+------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+------------------+-----------------+------------------+--------------------+\n",
      "|               V24|              V25|               V26|                 V27|\n",
      "+------------------+-----------------+------------------+--------------------+\n",
      "|0.0669280749146731|0.128539358273528|-0.189114843888824|   0.133558376740387|\n",
      "|-0.339846475529127|0.167170404418143| 0.125894532368176|-0.00898309914322813|\n",
      "+------------------+-----------------+------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------------+------+-----+\n",
      "|                V28|Amount|Class|\n",
      "+-------------------+------+-----+\n",
      "|-0.0210530534538215|149.62|    0|\n",
      "| 0.0147241691924927|  2.69|    0|\n",
      "+-------------------+------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_split(df, split=-1, n_samples=10):\n",
    "    n_cols = len(df.columns)\n",
    "    if split <= 0:\n",
    "        split = n_cols\n",
    "    i = 0\n",
    "    j = i + split\n",
    "    while i < n_cols:\n",
    "        df.select(*df.columns[i:j]).show(n_samples) # That * operator will unpack the columsn from [c1, c2, c3] to c1, c2, c3\n",
    "        i = j\n",
    "        j = i + split\n",
    "        \n",
    "show_split(df, 4, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bead69fb-e774-449c-bd26-5507a011b352",
   "metadata": {},
   "source": [
    "We can gather quick information about certain columns by typing a simple line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4907c3e3-d9c0-4412-bd34-eb29cbeebefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 8) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|summary|                 V27|                 V28|\n",
      "+-------+--------------------+--------------------+\n",
      "|  count|              284807|              284807|\n",
      "|   mean|-3.56859322007972...|-1.25938608605721...|\n",
      "| stddev|  0.4036324949650301|  0.3300832641602508|\n",
      "|    min|   -22.5656793207827|   -15.4300839055349|\n",
      "|    max|    31.6121981061363|    33.8478078188831|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.describe('V27', 'V28').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b793d9-aacc-4270-b8b7-a7d4b42cff21",
   "metadata": {},
   "source": [
    "We can be a little more specific with the values we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc58003a-1c36-430c-be56-470e6596de58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 8) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|summary|                 V25|                 V26|\n",
      "+-------+--------------------+--------------------+\n",
      "|  count|              284807|              284807|\n",
      "|    min|   -10.2953970749851|   -2.60455055280817|\n",
      "|    max|    7.51958867870916|     3.5173456116238|\n",
      "|   mean|7.153153300204557...|1.636403568872130...|\n",
      "|    50%|  0.0164864026203845| -0.0521920661334584|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# summary take statistics as params\n",
    "df.select('V25', 'V26').summary('count', 'min', 'max', 'mean', '50%').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323f0fa3-7023-4fba-b2f0-c93ab9157b71",
   "metadata": {},
   "source": [
    "As for today (04/2025), Python does not have access to the Dataset class; it can only use Dataframes. But, what is the difference between these two? And what makes them different to an RDD?\n",
    "* At the core, an RDD is an immutable distributed collection of elements of your data, partitioned across nodes in your cluster that can be operated in parallel with a low-level API that offers transformations and actions.\n",
    "  * They have no schema.\n",
    "* Like an RDD, a DataFrame is an immutable distributed collection of data. Unlike an RDD, data is organized into named columns, allowing higher-level abstraction. We could see these structures as DataFrames[Row]\n",
    "* Datasets are just strongly-typed DataFrames.\n",
    "\n",
    "You cannot overlook the space efficiency and performance gains in using DataFrames and Dataset APIs for two reasons.\n",
    "* First, because DataFrame and Dataset APIs are built on top of the Spark SQL engine, it uses Catalyst to generate an optimized logical and physical query plan. All relation type queries undergo the same code optimizer, providing the space and speed efficiency. Whereas the Dataset[T] typed API is optimized for data engineering tasks, the untyped Dataset[Row] (an alias of DataFrame) is even faster and suitable for interactive analysis.\n",
    "* Second, since Spark as a compiler understands your Dataset type JVM object, it maps your type-specific JVM object to Tungsten's internal memory representation using Encoders. As a result, Tungsten Encoders can efficiently serialize/deserialize JVM objects as well as generate compact bytecode that can execute at superior speeds.\n",
    "  * Tungsten is the codename for the umbrella project to make changes to Apache Spark’s execution engine that focuses on substantially improving the efficiency of memory and CPU for Spark applications\n",
    "\n",
    "From: https://www.databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd59a54-2196-42de-8463-77ae4730f394",
   "metadata": {},
   "source": [
    "# Explore the data in SQL-like fashion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66229fe2-0954-4e6c-ac49-59706ac5abb3",
   "metadata": {},
   "source": [
    "We can use PySpark-like syntax. In the example below, I access columns in three different ways. All of them work, although the third one (`F.col`) does require an import.\n",
    "What is the difference between them? https://stackoverflow.com/questions/55105363/pyspark-dataframe-column-reference-df-col-vs-dfcol-vs-f-colcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69828d45-5cd2-4178-8d35-03270375beba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "| Amount|Class|\n",
      "+-------+-----+\n",
      "| 149.62|    0|\n",
      "| 378.66|    0|\n",
      "|  123.5|    0|\n",
      "|  121.5|    0|\n",
      "| 231.71|    0|\n",
      "|1402.95|    0|\n",
      "| 120.96|    0|\n",
      "| 169.05|    0|\n",
      "| 243.66|    0|\n",
      "| 135.51|    0|\n",
      "+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df['Amount']>=100).select(df.Amount, F.col('Class')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f858c0-e4f1-4168-93a2-05be8f135421",
   "metadata": {},
   "source": [
    "We can also submit an SQL query directly.\n",
    "\n",
    "`createOrReplaceTempView()` creates (or replaces if that view name already exists) a lazily evaluated \"view\". We need it to run queries this way. It does not persist to memory unless you cache the dataset that underpins the view. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b91a98c4-4468-404f-b687-e7370e222ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "| Amount|Class|\n",
      "+-------+-----+\n",
      "| 149.62|    0|\n",
      "| 378.66|    0|\n",
      "|  123.5|    0|\n",
      "|  121.5|    0|\n",
      "| 231.71|    0|\n",
      "|1402.95|    0|\n",
      "| 120.96|    0|\n",
      "| 169.05|    0|\n",
      "| 243.66|    0|\n",
      "| 135.51|    0|\n",
      "+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('df')\n",
    "spark.sql('''\n",
    "    SELECT Amount, Class FROM df\n",
    "    WHERE AMount >= 100    \n",
    "''').show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7a1d05-042c-4ecc-9615-fab023cd3213",
   "metadata": {},
   "source": [
    "## Some more practice with more \"complex\" queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df60e9d7-f617-4352-a1af-a729dd4e5e16",
   "metadata": {},
   "source": [
    "Groups and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95ba3eb8-38bb-4b60-ae41-cab2e9b99fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|Class|           sumAmount|\n",
      "+-----+--------------------+\n",
      "|    0|1.9720083199999955E7|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(  \n",
    "  df.where(df['Amount']>=100) \n",
    "  .groupBy('Class') \n",
    "  .agg(F.sum('amount').alias('sumAmount')) # This agg() defines how the values should merge, but it does not imply an aggregation of the \n",
    "                                            # values per se (Could be a count, an avg, min, max, variance, etc.).\n",
    "  .where(F.col('sumAmount') > 300000)   # This second \"where\" acts like a HAVING (which in SQL acts as a where in aggregate functions anyway)\n",
    "  .show(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9dfa21-122d-4824-bc30-dd70bee3cf8b",
   "metadata": {},
   "source": [
    "Table unions (They work as a UNION ALL, with duplicates, unless stated otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61d2e7f6-442a-4431-a4ad-d292b8c9d891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "283726"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.union(df).distinct().count() # Maybe I need a better example for this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d2bfd3-ee75-4f21-9ae7-6d8aa79b0262",
   "metadata": {},
   "source": [
    "Table Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40996a56-bb43-437d-848b-3f1a5fb23b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe I need a better example for this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4a7401-d9ff-4dee-9070-596317c1c005",
   "metadata": {},
   "source": [
    "## UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7f788e-33b5-4b5a-a614-31b8525ca82d",
   "metadata": {},
   "source": [
    "What are UDFs? User-Defined Functions (aka UDF) is a feature of Spark SQL to define new Column-based functions that extend the vocabulary of Spark SQL's DSL for transforming Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbdf12be-773b-42d9-b77f-2638b32b57d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_amount_classes(amount:int) -> str:\n",
    "    if amount < 1000:\n",
    "        return \"Super low\"\n",
    "    if amount < 5000:\n",
    "        return \"Low\"\n",
    "    if amount < 10000:\n",
    "        return \"High\"\n",
    "    return \"Super high\"\n",
    "\n",
    "# Let's create a UDF\n",
    "giveAmountClasses = F.udf(give_amount_classes, T.StringType()) # Important!! Apparently, DataTypes are not singletons, so we have to \"create\" one by\n",
    "                                                    # calling the constructor i.e. adding those \"()\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589ed87f-2be2-4d7f-88cb-73e11eb61204",
   "metadata": {},
   "source": [
    "And what if we do not want to register the UDF manually? There is a tag for it as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8fdef5a5-4abb-4b1d-89a7-ff353c24395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(T.StringType())\n",
    "def myClassName(myClass:int) -> str:\n",
    "    if myClass == 0:\n",
    "        return \"Class 0\"\n",
    "    return \"Class 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79603655-376c-4caf-8ab8-f8143d6f9bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+-------------+\n",
      "|Time|AmountString|myClassString|\n",
      "+----+------------+-------------+\n",
      "| 0.0|   Super low|      Class 0|\n",
      "| 0.0|   Super low|      Class 0|\n",
      "| 1.0|   Super low|      Class 0|\n",
      "| 1.0|   Super low|      Class 0|\n",
      "| 2.0|   Super low|      Class 0|\n",
      "| 2.0|   Super low|      Class 0|\n",
      "| 4.0|   Super low|      Class 0|\n",
      "| 7.0|   Super low|      Class 0|\n",
      "| 7.0|   Super low|      Class 0|\n",
      "| 9.0|   Super low|      Class 0|\n",
      "+----+------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Time', giveAmountClasses(df.Amount).alias('AmountString'), myClassName(df.Class).alias(\"myClassString\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1bf4a7ef-124e-447b-b3b7-c14840453ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|    Time|AmountString|\n",
      "+--------+------------+\n",
      "| 42951.0|  Super high|\n",
      "| 46253.0|  Super high|\n",
      "| 48401.0|  Super high|\n",
      "| 95286.0|  Super high|\n",
      "|119713.0|  Super high|\n",
      "|145283.0|  Super high|\n",
      "|166198.0|  Super high|\n",
      "|172273.0|  Super high|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's make sure we have other values of \"AmountString\" too.\n",
    "# TODO Cache the above table so we don't have to recompute it here\n",
    "df.select('Time', giveAmountClasses(df.Amount).alias('AmountString')).where(F.col(\"AmountString\") == \"Super high\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a337ae2-b372-4c9c-8f5c-bc597d0dd05e",
   "metadata": {},
   "source": [
    "What if we want to use this UDF with the SQL syntax? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "58a75be0-8e78-4ab1-8985-1df5f78406f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+\n",
      "|Time|AmountString|\n",
      "+----+------------+\n",
      "| 0.0|   Super low|\n",
      "| 0.0|   Super low|\n",
      "| 1.0|   Super low|\n",
      "| 1.0|   Super low|\n",
      "| 2.0|   Super low|\n",
      "| 2.0|   Super low|\n",
      "| 4.0|   Super low|\n",
      "| 7.0|   Super low|\n",
      "| 7.0|   Super low|\n",
      "| 9.0|   Super low|\n",
      "+----+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register('giveAmountClasses', give_amount_classes, T.StringType()) # We need to register the function with this one\n",
    "                                                                            # The F.udf used above won't cut it \n",
    "spark.sql('''\n",
    "    SELECT Time, giveAmountClasses(amount) AmountString\n",
    "    FROM df\n",
    "''').show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2df1ab-4c73-489b-89d1-a9cfaad3f75b",
   "metadata": {},
   "source": [
    "### Pandas UDF vs Python UDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85876a93-fdd1-4101-92d6-24e6398815b8",
   "metadata": {},
   "source": [
    "* In a Python UDF, when you pass column objects to your UDF, PySpark will unpack each value/row (convert the data between the python environment and the JVM, basically a serialization), perform the computation, and then return the value for each record.\n",
    "* In a Scalar UDF, PySpark will serialize (through a library called PyArrow) each partitioned column into a pandas Series object. You then perform the operations on the Series object directly (avoiding the overhead of unpacking each row individually),returning a Series of the same dimension from your UDF.\n",
    "\n",
    "From an end user perspective, they are the same functionally. Because pandas is optimized for rapid data manipulation, it is preferable to use a Series to Series UDF when you can instead of using a regular Python UDF, as it’ll be much faster.\n",
    "\n",
    "Of course, this is only a problem for PySpark. If we were to use Spark and Scala/Java, that would be no problem.\n",
    "\n",
    "From: https://gist.github.com/ThaiDat/81c3662801aa8410a65b94f3c993c377"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9113d4da-72f7-443e-ae11-9498881ada4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(T.StringType())\n",
    "def getUserType(myClass: pd.Series) -> pd.Series:\n",
    "    if myClass == 0:\n",
    "        return \"Class 0\"\n",
    "    return \"Class 1\"\n",
    "\n",
    "# We can also promote the function to pandas as GetUserType = F.pandas_udf(get_user_type, T.StringType())\n",
    "def give_amount_classes(amount:pd.Series) -> pd.Series:\n",
    "    if amount < 1000:\n",
    "        return \"Super low\"\n",
    "    if amount < 5000:\n",
    "        return \"Low\"\n",
    "    if amount < 10000:\n",
    "        return \"High\"\n",
    "    return \"Super high\"\n",
    "\n",
    "giveAmountClasses = F.udf(give_amount_classes, T.StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5535b1f0-8698-4cc5-add6-4fe32cb443f8",
   "metadata": {},
   "source": [
    "We cannot use these UDFs with the SQL syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "870b1197-aea0-40c3-9a32-bb2bf971df67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+-------------+\n",
      "|Time|AmountString|myClassString|\n",
      "+----+------------+-------------+\n",
      "| 0.0|   Super low|      Class 0|\n",
      "| 0.0|   Super low|      Class 0|\n",
      "| 1.0|   Super low|      Class 0|\n",
      "| 1.0|   Super low|      Class 0|\n",
      "| 2.0|   Super low|      Class 0|\n",
      "| 2.0|   Super low|      Class 0|\n",
      "| 4.0|   Super low|      Class 0|\n",
      "| 7.0|   Super low|      Class 0|\n",
      "| 7.0|   Super low|      Class 0|\n",
      "| 9.0|   Super low|      Class 0|\n",
      "+----+------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Time', giveAmountClasses(df.Amount).alias('AmountString'), myClassName(df.Class).alias(\"myClassString\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c37f7298-3053-4b90-9116-98f078966367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|    Time|AmountString|\n",
      "+--------+------------+\n",
      "| 42951.0|  Super high|\n",
      "| 46253.0|  Super high|\n",
      "| 48401.0|  Super high|\n",
      "| 95286.0|  Super high|\n",
      "|119713.0|  Super high|\n",
      "|145283.0|  Super high|\n",
      "|166198.0|  Super high|\n",
      "|172273.0|  Super high|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's make sure we have other values of \"AmountString\" too.\n",
    "# TODO Cache the above table so we don't have to recompute it here\n",
    "df.select('Time', giveAmountClasses(df.Amount).alias('AmountString')).where(F.col(\"AmountString\") == \"Super high\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2505d3fb-f2cd-4ef8-b4e8-6ac54bc94b14",
   "metadata": {},
   "source": [
    "TODO: Is there an UDF that could not be transformed into a Pandas' UDF?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44eb6f-be6e-472b-abf5-fe417543bf85",
   "metadata": {},
   "source": [
    "Now, what if ,instead of passing a column to our UDF, we want to pass it two or three? In this case things are a little bit different.@pandas_udf(returnType=T.FloatType())\n",
    "def tip_percent_of_fare(fares_breakdown: pd.DataFrame) -> pd.Series:\n",
    "    print(fares_breakdown)\n",
    "    result = (fares_breakdown['tip_amount']/fares_breakdown['total_amount'] * 100)\n",
    "    print(\"!============================================!\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f95f1be-62ff-44c0-b644-2ac7a78bf11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(returnType=T.FloatType())\n",
    "def tip_percent_of_fare(fares_breakdown: pd.DataFrame) -> pd.Series:   # pd.DataFrame is the important difference here\n",
    "    print(fares_breakdown)\n",
    "    result = (fares_breakdown['tip_amount']/fares_breakdown['total_amount'] * 100)\n",
    "    print(\"!============================================!\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92526e4c-6bb2-4f29-81bd-dbca451f2cec",
   "metadata": {},
   "source": [
    "These UDFs are processed in batches. What if there is an operation we do not want to repeat over an over every batch? What if we only want to do it once and use the result of said operation every batch? (loading a ML model, for example). In this case, instead of defining the input and output of our UDF as a pd.Series, we could define it as an iterator of batches of pd.Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e503e0c0-6c94-4a16-b9e5-b5ef8e5c2f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"long\")\n",
    "def square_plus_y(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    y = y_bc.value  # initialize states\n",
    "    try:\n",
    "        for x in batch_iter:\n",
    "            yield x * x + y\n",
    "    finally:\n",
    "        pass  # release resources here, if anytest_df.select(square_plus_y(col(\"id\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb40700-4fe7-41e5-8e54-f5799c31e9fc",
   "metadata": {},
   "source": [
    "And what if we need to use this solution but use several columns as input? No problem either:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1c0afe-4f53-4f3d-8936-f2b7087c447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amount_mismatch(values: Iterator[Tuple[pd.Series, pd.Series, pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
    "    # Heavy task\n",
    "    # ...\n",
    "    \n",
    "    for oldOrig, newOrig, oldDest, newDest in values:\n",
    "        yield abs(abs(newOrig - oldOrig) - abs(newDest - oldDest))\n",
    "        \n",
    "amountMismatch = F.pandas_udf(amount_mismatch, T.DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966c4e03-e698-4475-a06c-c81a41cf2c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "All the operations above return a series of the same lenght of the input. What if we want something different? We can also do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2353829-de55-4f3a-ab15-9a64eaf61925",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can return only one value per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96a768d-2e9d-4ac0-8716-813d4da6a499",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(T.DoubleType())\n",
    "def GetStdDeviation(series: pd.Series) -> float:\n",
    "    return series.std()\n",
    "\n",
    "(\n",
    "    df.groupBy('type')\n",
    "    .agg(\n",
    "        GetStdDeviation(df.amount).alias('var')\n",
    "    )\n",
    "    .orderBy('var', ascending=False)\n",
    "    .show(10)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad6bd6e-d54f-45e3-8181-3acb96a17259",
   "metadata": {},
   "outputs": [],
   "source": [
    "Or a completely new struck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e83723-a0f3-464a-9fcf-fc6d06a2a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_by_type(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    result = data[['type', 'amount']].copy()\n",
    "    maxVal = result['amount'].max()\n",
    "    minVal = result['amount'].min()\n",
    "    if maxVal == minVal:\n",
    "        result['amountNorm'] = 0.5\n",
    "    else:\n",
    "        result['amountNorm'] = (result['amount'] - minVal) / (maxVal - minVal)\n",
    "    return result\n",
    "\n",
    "# We can use the SQL version of schema: \n",
    "# schema = 'type string, amount double, amountNorm double'\n",
    "schema = T.StructType([\n",
    "    T.StructField('type', T.StringType()),\n",
    "    T.StructField('amount', T.DoubleType()),\n",
    "    T.StructField('amountNorm', T.DoubleType())\n",
    "])\n",
    "\n",
    "(\n",
    "    df.groupBy('type')\n",
    "    .applyInPandas(normalize_by_type, schema)   # When using this function, we do not need to define the method as an UDF.\n",
    "                                                # IMPORTANT! Apparently, this is now recommended in Spark 3 and the older method deprecated! \n",
    "                                                # But it can only be used when dealing with pd.DataFrame as inputs and outputs\n",
    "    .show(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d401903a-5527-4b7e-803c-5e60c870d33a",
   "metadata": {},
   "source": [
    "The function you need to use depends on the output:\n",
    "\n",
    "| Input | Output | Function |\n",
    "| --- | --- | --- |\n",
    "| One row | One row | Pandas UDF |\n",
    "| Many rows | One | `applyInPandas` |\n",
    "| One row | Many rows | `mapInPandas()` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8458129-fb2e-43bc-b3bb-9a5201e88079",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce11c80-a150-4080-a87c-a9c574eefbce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028f4005-6f15-426f-95e3-15fe73e0e7d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c18dfb2-587b-469a-995d-0592c919d571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05f44bb-9867-48ab-8e9e-85dfe8221ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e406c081-31e3-4718-81fc-e9ebc27faa83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6755794e-8453-46c6-bee9-bc3dc42fd504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
