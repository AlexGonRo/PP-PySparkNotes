{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da3c717-1f4a-41eb-8e5d-97b327fc8a7c",
   "metadata": {},
   "source": [
    "# PYSPARK PRACTICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f762d-88d2-46eb-9daf-a14b10007649",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceea45b7-33bb-47f4-bf87-8c30512b1ddc",
   "metadata": {},
   "source": [
    "We'll be working with high-level operations, so we need to create a SparkSession (a unified entrypoint for Spark that contains SparkContext, StreamingContext and SQLContext). If we wanted to work directly with RDDs, using a SparkContext would be more appropiate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd9e20-cc4b-479f-b436-c6a1b9345348",
   "metadata": {},
   "source": [
    "// TODO I'd like to test SparkContext at some point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc040af5-71d4-4565-a41e-d5f99273008d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/19 15:21:40 WARN Utils: Your hostname, alejandro resolves to a loopback address: 127.0.1.1; using 192.168.3.37 instead (on interface wlp2s0)\n",
      "25/04/19 15:21:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/19 15:21:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/19 15:21:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.3.37:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Demo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7e2caf5fd880>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "   SparkSession.builder.appName('Spark Demo') \n",
    "  .master('local[*]') #local[*] - Run Spark locally with as many worker threads as logical cores on your machine. I could choose any number really\n",
    "                    # YARN when deploying to a cluster with YARN\n",
    "  .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"100\")  # For testing purposes\n",
    "  .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff888385-053d-49fe-854c-2494236346ca",
   "metadata": {},
   "source": [
    "I won´t be using it here, but I should make an honorary mention to spark-submit, the script used to deploy jobs in a Spark cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df4e4a-e344-47c9-91b9-0eb8b072380a",
   "metadata": {},
   "source": [
    "```bash\n",
    "./bin/spark-submit \\\n",
    "  --class <main-class> \\\n",
    "  --master <master-url> \\\n",
    "  --deploy-mode <deploy-mode> \\\n",
    "  --conf <key>=<value> \\\n",
    "  --py-files file1.py,file2.py,file3.zip,file4.egg \\ # These will be passed to the worker nodes\n",
    "  --archives conda_env.tag.gz \\ # We could package the whole conda environment if we wanted to. Or a virtualEnv \n",
    "  ... # other options\n",
    "  <application-jar> \\\n",
    "  [application-arguments]\n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ddd712-b94e-447b-a143-7c243c5aeb38",
   "metadata": {},
   "source": [
    "Before we continue, some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "095cbd0c-f400-4c8d-a373-a328ba243579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.functions import lit\n",
    "from typing import Iterator, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c24357-dce0-460b-a6c4-db4c02590914",
   "metadata": {},
   "source": [
    "## Explore the data - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e970084-8d38-41a3-9732-953557b2f167",
   "metadata": {},
   "source": [
    "Let's just load a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "671c38c0-71b2-40d3-bf26-c41e7cff070c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- trip_type: long (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path_green = './data/green_tripdata_2025-01.parquet'\n",
    "data_path_yellow = './data/yellow_tripdata_2025-01.parquet'\n",
    "df_green = spark.read.parquet(data_path_green, header=True, inferSchema=True) # It is reading the file from my local file system, not the master or worker nodes\n",
    "df_yellow = spark.read.parquet(data_path_yellow, header=True, inferSchema=True) # It is reading the file from my local file system, not the master or worker nodes\n",
    "display(df_green.printSchema()) # That display() is part of Jupyter to show the output\n",
    "display(df_yellow.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f544efb8-f09e-4822-b53f-5310f866920d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|       2| 2025-01-01 00:03:01|  2025-01-01 00:17:12|                 N|         1|          75|         235|              1|         5.93|       24.7|  1.0|    0.5|       6.8|         0.0|     NULL|                  1.0|        34.0|           1|        1|                 0.0|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169ed589-4acb-4839-aca4-f75466f4f499",
   "metadata": {},
   "source": [
    "What if I want to manually create a DF? Let's do it here and use it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c728417-4572-4466-967e-a548edde6a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------+\n",
      "| id|           name|country|\n",
      "+---+---------------+-------+\n",
      "|  1|   Luis Mendoza| Mexico|\n",
      "|  2|    Amina Yusuf|Nigeria|\n",
      "|  3|       Chen Wei|  China|\n",
      "|  4|  Elena Petrova| Russia|\n",
      "|  5| David O'Connor|Ireland|\n",
      "|  6|      Raj Patel|  India|\n",
      "|  7|Fatima Al-Sayed|  Egypt|\n",
      "|  8|     John Smith|    USA|\n",
      "|  9|   Mateo García|  Spain|\n",
      "| 10|    Anna Müller|Germany|\n",
      "+---+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Luis Mendoza\", \"Mexico\"),\n",
    "    (2, \"Amina Yusuf\", \"Nigeria\"),\n",
    "    (3, \"Chen Wei\", \"China\"),\n",
    "    (4, \"Elena Petrova\", \"Russia\"),\n",
    "    (5, \"David O'Connor\", \"Ireland\"),\n",
    "    (6, \"Raj Patel\", \"India\"),\n",
    "    (7, \"Fatima Al-Sayed\", \"Egypt\"),\n",
    "    (8, \"John Smith\", \"USA\"),\n",
    "    (9, \"Mateo García\", \"Spain\"),\n",
    "    (10, \"Anna Müller\", \"Germany\")\n",
    "]\n",
    "\n",
    "# Manually define schema\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"id\", T.IntegerType(), False),\n",
    "    T.StructField(\"name\", T.StringType(), False),\n",
    "    T.StructField(\"country\", T.StringType(), False)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df_drivers = spark.createDataFrame(data, schema)\n",
    "df_drivers.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d46fb5-85bd-422f-9a3d-d6decb29d187",
   "metadata": {},
   "source": [
    "Three important terms to remember here:\n",
    "* *StructType*: A built-in DataType from org.apache.spark.sql.types that implements scala.collection.Seq<StructField>. Basically, it is a `Seq` of `StructField`. \n",
    "* *StructField*: The name, type and default `null` value of a row. You could see it as metadata of whatever you'll find in a row.\n",
    "* *Row*: The values of the \"column\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fbf838-0857-456e-870d-ec530bb97469",
   "metadata": {},
   "source": [
    "Let's do something a little bit more complicated. Let us show columns in groups of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76353eb3-e4ee-45b3-9d56-d29e3c26b080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|\n",
      "+--------+--------------------+---------------------+------------------+\n",
      "|       2| 2025-01-01 00:03:01|  2025-01-01 00:17:12|                 N|\n",
      "|       2| 2025-01-01 00:19:59|  2025-01-01 00:25:52|                 N|\n",
      "+--------+--------------------+---------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+------------+------------+---------------+\n",
      "|RatecodeID|PULocationID|DOLocationID|passenger_count|\n",
      "+----------+------------+------------+---------------+\n",
      "|         1|          75|         235|              1|\n",
      "|         1|         166|          75|              1|\n",
      "+----------+------------+------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------+-----------+-----+-------+\n",
      "|trip_distance|fare_amount|extra|mta_tax|\n",
      "+-------------+-----------+-----+-------+\n",
      "|         5.93|       24.7|  1.0|    0.5|\n",
      "|         1.32|        8.6|  1.0|    0.5|\n",
      "+-------------+-----------+-----+-------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+------------+---------+---------------------+\n",
      "|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|\n",
      "+----------+------------+---------+---------------------+\n",
      "|       6.8|         0.0|     NULL|                  1.0|\n",
      "|       0.0|         0.0|     NULL|                  1.0|\n",
      "+----------+------------+---------+---------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+------------+------------+---------+--------------------+\n",
      "|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+------------+------------+---------+--------------------+\n",
      "|        34.0|           1|        1|                 0.0|\n",
      "|        11.1|           2|        1|                 0.0|\n",
      "+------------+------------+---------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_split(df, split=-1, n_samples=10):\n",
    "    n_cols = len(df.columns)\n",
    "    if split <= 0:\n",
    "        split = n_cols\n",
    "    i = 0\n",
    "    j = i + split\n",
    "    while i < n_cols:\n",
    "        df.select(*df.columns[i:j]).show(n_samples) # That * operator will unpack the columsn from [c1, c2, c3] to c1, c2, c3\n",
    "        i = j\n",
    "        j = i + split\n",
    "        \n",
    "show_split(df_green, 4, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bead69fb-e774-449c-bd26-5507a011b352",
   "metadata": {},
   "source": [
    "We can gather quick information about certain columns by typing a simple line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4907c3e3-d9c0-4412-bd34-eb29cbeebefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+\n",
      "|summary|        tip_amount|       tolls_amount|\n",
      "+-------+------------------+-------------------+\n",
      "|  count|             48326|              48326|\n",
      "|   mean|2.4818590406821954|0.17746099408185748|\n",
      "| stddev| 3.213612255803839| 1.1929839044404538|\n",
      "|    min|              -0.9|                0.0|\n",
      "|    max|            252.05|              48.76|\n",
      "+-------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green.describe('tip_amount', 'tolls_amount').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b793d9-aacc-4270-b8b7-a7d4b42cff21",
   "metadata": {},
   "source": [
    "We can be a little more specific with the values we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc58003a-1c36-430c-be56-470e6596de58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|       fare_amount|             extra|\n",
      "+-------+------------------+------------------+\n",
      "|  count|             48326|             48326|\n",
      "|    min|            -113.0|              -5.0|\n",
      "|    max|             336.2|               7.5|\n",
      "|   mean|16.762465546496703|0.9323242147084385|\n",
      "|    50%|              13.5|               0.0|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# summary take statistics as params\n",
    "df_green.select('fare_amount', 'extra').summary('count', 'min', 'max', 'mean', '50%').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323f0fa3-7023-4fba-b2f0-c93ab9157b71",
   "metadata": {},
   "source": [
    "As for today (04/2025), Python does not have access to the Dataset class; it can only use Dataframes. But, what is the difference between these two? And what makes them different to an RDD?\n",
    "* At the core, an RDD is an immutable distributed collection of elements of your data, partitioned across nodes in your cluster that can be operated in parallel with a low-level API that offers transformations and actions.\n",
    "  * They have no schema.\n",
    "* Like an RDD, a DataFrame is an immutable distributed collection of data. Unlike an RDD, data is organized into named columns, allowing higher-level abstraction. We could see these structures as DataFrames[Row]\n",
    "* Datasets are just strongly-typed DataFrames.\n",
    "\n",
    "You cannot overlook the space efficiency and performance gains in using DataFrames and Dataset APIs for two reasons.\n",
    "* First, because DataFrame and Dataset APIs are built on top of the Spark SQL engine, it uses Catalyst to generate an optimized logical and physical query plan. All relation type queries undergo the same code optimizer, providing the space and speed efficiency. Whereas the Dataset[T] typed API is optimized for data engineering tasks, the untyped Dataset[Row] (an alias of DataFrame) is even faster and suitable for interactive analysis.\n",
    "* Second, since Spark as a compiler understands your Dataset type JVM object, it maps your type-specific JVM object to Tungsten's internal memory representation using Encoders. As a result, Tungsten Encoders can efficiently serialize/deserialize JVM objects as well as generate compact bytecode that can execute at superior speeds.\n",
    "  * Tungsten is the codename for the umbrella project to make changes to Apache Spark’s execution engine that focuses on substantially improving the efficiency of memory and CPU for Spark applications\n",
    "\n",
    "From: https://www.databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f6978-3c8c-4ea9-8124-d58144f217cf",
   "metadata": {},
   "source": [
    "Let's keep messing with our datasets. Now I want to add (and remove) a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "058aac34-6df2-442f-8186-58fd0927a8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: int, lpep_pickup_datetime: timestamp_ntz, lpep_dropoff_datetime: timestamp_ntz, store_and_fwd_flag: string, RatecodeID: bigint, PULocationID: int, DOLocationID: int, passenger_count: bigint, trip_distance: double, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, ehail_fee: double, improvement_surcharge: double, total_amount: double, payment_type: bigint, trip_type: bigint, congestion_surcharge: double]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.withColumn(\"tmp_column\", lit(10)) # That function lit() is very important, Spark will fail without it. It means \"literal\"\n",
    "                                            # and it is a way to tell Spark that that is the literal value that should be in every record\n",
    "df_green.drop('tmp_column')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb5b1b-b874-421d-89f4-708e1070e42f",
   "metadata": {},
   "source": [
    "And now I want to change the value of a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50e298a4-34bd-4acc-83a0-9ca2f1ec61a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_green.withColumn(\"passenger_count\", lit(2)) # The name of the column already exists, so no need to add new column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd59a54-2196-42de-8463-77ae4730f394",
   "metadata": {},
   "source": [
    "# Explore the data in SQL-like fashion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66229fe2-0954-4e6c-ac49-59706ac5abb3",
   "metadata": {},
   "source": [
    "We can use PySpark-like syntax. In the example below, I access columns in three different ways. All of them work, although the third one (`F.col`) does require an import.\n",
    "What is the difference between them? https://stackoverflow.com/questions/55105363/pyspark-dataframe-column-reference-df-col-vs-dfcol-vs-f-colcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69828d45-5cd2-4178-8d35-03270375beba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "|total_amount|trip_type|\n",
      "+------------+---------+\n",
      "|       108.9|        2|\n",
      "|       101.0|        2|\n",
      "|       222.0|        2|\n",
      "|      164.38|        2|\n",
      "|       124.5|        2|\n",
      "|       120.0|        2|\n",
      "|       114.0|        1|\n",
      "|      121.28|        2|\n",
      "|       110.9|        1|\n",
      "|       103.2|        2|\n",
      "+------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green.where(df_green['total_amount']>=100).select(df_green.total_amount, F.col('trip_type')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f858c0-e4f1-4168-93a2-05be8f135421",
   "metadata": {},
   "source": [
    "We can also submit an SQL query directly.\n",
    "\n",
    "`createOrReplaceTempView()` creates (or replaces if that view name already exists) a lazily evaluated \"view\". We need it to run queries this way. It does not persist to memory unless you cache the dataset that underpins the view. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b91a98c4-4468-404f-b687-e7370e222ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "|total_amount|trip_type|\n",
      "+------------+---------+\n",
      "|       108.9|        2|\n",
      "|       101.0|        2|\n",
      "|       222.0|        2|\n",
      "|      164.38|        2|\n",
      "|       124.5|        2|\n",
      "|       120.0|        2|\n",
      "|       114.0|        1|\n",
      "|      121.28|        2|\n",
      "|       110.9|        1|\n",
      "|       103.2|        2|\n",
      "+------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green.createOrReplaceTempView('df') # Register the dataframe as a view. This operation is lazy, won't materialize until we need it.\n",
    "                                        # It does not cache unless otherwise specified.\n",
    "spark.sql('''\n",
    "    SELECT total_amount, trip_type FROM df\n",
    "    WHERE total_amount >= 100 \n",
    "''').show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135c5b9b-47ea-4ef3-9377-0619be4f95c2",
   "metadata": {},
   "source": [
    "If I modify the underlaying df, would hte view be affected by it? Let's have a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc5230e1-9d85-4a94-94f3-6dae58d3faa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: int, lpep_pickup_datetime: timestamp_ntz, lpep_dropoff_datetime: timestamp_ntz, store_and_fwd_flag: string, RatecodeID: bigint, PULocationID: int, DOLocationID: int, passenger_count: bigint, trip_distance: double, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, ehail_fee: double, improvement_surcharge: double, total_amount: double, payment_type: bigint, trip_type: bigint, congestion_surcharge: double]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.withColumn(\"tmp_column\", lit(10))\n",
    "\n",
    "#spark.sql('''\n",
    "#    SELECT total_amount, trip_type, tmp_column FROM df\n",
    "#    WHERE total_amount >= 100 \n",
    "#''').show(10)\n",
    "\n",
    "## It fails because it does not find the new column!\n",
    "\n",
    "df_green.drop('tmp_column')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7a1d05-042c-4ecc-9615-fab023cd3213",
   "metadata": {},
   "source": [
    "## Some more practice with more \"complex\" queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df60e9d7-f617-4352-a1af-a729dd4e5e16",
   "metadata": {},
   "source": [
    "Groups and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95ba3eb8-38bb-4b60-ae41-cab2e9b99fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|VendorID|        sumAmount|\n",
      "+--------+-----------------+\n",
      "|       2|954358.4599999995|\n",
      "+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Out of the first 50 vendors, which ones do have a total amount bigger than a given number?\n",
    "(  \n",
    "  df_green.where(df_green['VendorID']<=50) \n",
    "  .groupBy('VendorID') \n",
    "  .agg(F.sum('total_amount').alias('sumAmount')) # This agg() defines how the values should merge, but it does not imply an aggregation of the \n",
    "                                            # values per se (Could be a count, an avg, min, max, variance, etc.).\n",
    "  .where(F.col('sumAmount') > 300000)   # This second \"where\" acts like a HAVING (which in SQL acts as a where in aggregate functions anyway)\n",
    "  .show(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9dfa21-122d-4824-bc30-dd70bee3cf8b",
   "metadata": {},
   "source": [
    "Table unions (They work as a UNION ALL, with duplicates, unless stated otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61d2e7f6-442a-4431-a4ad-d292b8c9d891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48326"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "24147"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "72473"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "48326"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_split = df_green.randomSplit([0.5, 0.5])[0]\n",
    "display(df_green.count())\n",
    "display(df_split.count())  \n",
    "display(df_green.union(df_split).count()) # Duplicates are still there!\n",
    "display(df_green.union(df_split).distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d2bfd3-ee75-4f21-9ae7-6d8aa79b0262",
   "metadata": {},
   "source": [
    "Table Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40996a56-bb43-437d-848b-3f1a5fb23b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------+-------+\n",
      "|VendorID|total_amount|        name|country|\n",
      "+--------+------------+------------+-------+\n",
      "|       1|        57.2|Luis Mendoza| Mexico|\n",
      "|       1|       13.49|Luis Mendoza| Mexico|\n",
      "|       1|       16.95|Luis Mendoza| Mexico|\n",
      "|       1|       59.57|Luis Mendoza| Mexico|\n",
      "|       1|       17.65|Luis Mendoza| Mexico|\n",
      "|       1|        18.8|Luis Mendoza| Mexico|\n",
      "|       1|       90.21|Luis Mendoza| Mexico|\n",
      "|       1|       17.94|Luis Mendoza| Mexico|\n",
      "|       1|        29.7|Luis Mendoza| Mexico|\n",
      "|       1|       16.12|Luis Mendoza| Mexico|\n",
      "+--------+------------+------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Maybe I need a better example for this\n",
    "(\n",
    "df_green.where(df_green[\"VendorID\"]<=10)\n",
    ".join(df_drivers, df_green.VendorID == df_drivers.id, \"inner\")\n",
    ".select(F.col('VendorID'), F.col('total_amount'), F.col('name'), F.col('country'))\n",
    ".show(10)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4a7401-d9ff-4dee-9070-596317c1c005",
   "metadata": {},
   "source": [
    "## UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7f788e-33b5-4b5a-a614-31b8525ca82d",
   "metadata": {},
   "source": [
    "What are UDFs? User-Defined Functions (aka UDF) is a feature of Spark SQL to define new Column-based functions that extend the vocabulary of Spark SQL's DSL for transforming Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7ff3382-3dcd-4e5c-81e8-157afc335b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|payment_type|\n",
      "+------------+\n",
      "|           5|\n",
      "|           1|\n",
      "|           3|\n",
      "|           2|\n",
      "|           4|\n",
      "|        NULL|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green.select(df_green['payment_type']).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbdf12be-773b-42d9-b77f-2638b32b57d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_payment_name(payment_type:int) -> str:\n",
    "    if payment_type == 1:\n",
    "        return \"Cash\"\n",
    "    if payment_type == 2:\n",
    "        return \"National_Card\"\n",
    "    if payment_type == 3:\n",
    "        return \"International_Card\"\n",
    "    if payment_type == 4:\n",
    "        return \"Bizum\"\n",
    "    if payment_type == 5:\n",
    "        return \"Begged_for_mercy\"\n",
    "    return \"Null?\"\n",
    "\n",
    "# Let's create a UDF\n",
    "givePaymentName = F.udf(give_payment_name, T.StringType()) # Important!! Apparently, DataTypes are not singletons, so we have to \"create\" one by\n",
    "                                                    # calling the constructor i.e. adding those \"()\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589ed87f-2be2-4d7f-88cb-73e11eb61204",
   "metadata": {},
   "source": [
    "And what if we do not want to register the UDF manually? There is a tag for it as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fdef5a5-4abb-4b1d-89a7-ff353c24395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(T.StringType())\n",
    "def givePaymentName(payment_type:int) -> str:\n",
    "    if payment_type == 1:\n",
    "        return \"Cash\"\n",
    "    if payment_type == 2:\n",
    "        return \"National_Card\"\n",
    "    if payment_type == 3:\n",
    "        return \"International_Card\"\n",
    "    if payment_type == 4:\n",
    "        return \"Bizum\"\n",
    "    if payment_type == 5:\n",
    "        return \"Begged_for_mercy\"\n",
    "    return \"Null?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79603655-376c-4caf-8ab8-f8143d6f9bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------------+\n",
      "|VendorID|givePaymentName(payment_type)|\n",
      "+--------+-----------------------------+\n",
      "|       2|                         Cash|\n",
      "|       2|                National_Card|\n",
      "|       2|                National_Card|\n",
      "|       2|                         Cash|\n",
      "|       2|                         Cash|\n",
      "|       2|                National_Card|\n",
      "|       2|                         Cash|\n",
      "|       2|                National_Card|\n",
      "|       2|                         Cash|\n",
      "|       2|                National_Card|\n",
      "+--------+-----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_green.select('VendorID', givePaymentName(df_green.payment_type)).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a337ae2-b372-4c9c-8f5c-bc597d0dd05e",
   "metadata": {},
   "source": [
    "What if we want to use this UDF with the SQL syntax? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58a75be0-8e78-4ab1-8985-1df5f78406f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|VendorID|PaymentTypeString|\n",
      "+--------+-----------------+\n",
      "|       2|             Cash|\n",
      "|       2|    National_Card|\n",
      "|       2|    National_Card|\n",
      "|       2|             Cash|\n",
      "|       2|             Cash|\n",
      "|       2|    National_Card|\n",
      "|       2|             Cash|\n",
      "|       2|    National_Card|\n",
      "|       2|             Cash|\n",
      "|       2|    National_Card|\n",
      "+--------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register('givePaymentName', give_payment_name, T.StringType()) # We need to register the function with this one\n",
    "                                                                            # The F.udf used above won't cut it \n",
    "spark.sql('''\n",
    "    SELECT VendorID, givePaymentName(payment_type) PaymentTypeString\n",
    "    FROM df\n",
    "''').show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2df1ab-4c73-489b-89d1-a9cfaad3f75b",
   "metadata": {},
   "source": [
    "### Pandas UDF vs Python UDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85876a93-fdd1-4101-92d6-24e6398815b8",
   "metadata": {},
   "source": [
    "* In a Python UDF, when you pass column objects to your UDF, PySpark will unpack each value/row (convert the data between the python environment and the JVM, basically a serialization), perform the computation, and then return the value for each record.\n",
    "* In a Scalar UDF, PySpark will serialize (through a library called PyArrow) each partitioned column into a pandas Series object. You then perform the operations on the Series object directly (avoiding the overhead of unpacking each row individually),returning a Series of the same dimension from your UDF.\n",
    "\n",
    "From an end user perspective, they are the same functionally. Because pandas is optimized for rapid data manipulation, it is preferable to use a Series to Series UDF when you can instead of using a regular Python UDF, as it’ll be much faster.\n",
    "\n",
    "Of course, this is only a problem for PySpark. If we were to use Spark and Scala/Java, that would be no problem.\n",
    "\n",
    "From: https://gist.github.com/ThaiDat/81c3662801aa8410a65b94f3c993c377"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9113d4da-72f7-443e-ae11-9498881ada4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.pandas_udf(T.StringType())\n",
    "def giveTripTypeName(trip_type: pd.Series) -> pd.Series:\n",
    "    return trip_type.map({\n",
    "        1: \"Normal\",\n",
    "        2: \"Premium\"\n",
    "    }).fillna(\"Null?\")\n",
    "\n",
    "# We can also promote the function to pandas as GetUserType = F.pandas_udf(get_user_type, T.StringType())\n",
    "def give_payment_name(payment_type:pd.Series) -> pd.Series:   # We are overwritting the method from above\n",
    "    return payment_type.map({\n",
    "        1: \"Cash\",\n",
    "        2: \"National_Card\",\n",
    "        3: \"International_Card\",\n",
    "        4: \"Bizum\",\n",
    "        5: \"Begged_for_mercy\"\n",
    "    }).fillna(\"Null?\")\n",
    "\n",
    "\n",
    "givePaymentName = F.pandas_udf(give_payment_name, T.StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5535b1f0-8698-4cc5-add6-4fe32cb443f8",
   "metadata": {},
   "source": [
    "We cannot use these UDFs with the SQL syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "870b1197-aea0-40c3-9a32-bb2bf971df67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-------------+\n",
      "|VendorID|PaymentTypeName|TripTypeClass|\n",
      "+--------+---------------+-------------+\n",
      "|       2|           Cash|       Normal|\n",
      "|       2|  National_Card|       Normal|\n",
      "|       2|  National_Card|      Premium|\n",
      "|       2|           Cash|       Normal|\n",
      "|       2|           Cash|       Normal|\n",
      "|       2|  National_Card|       Normal|\n",
      "|       2|           Cash|       Normal|\n",
      "|       2|  National_Card|       Normal|\n",
      "|       2|           Cash|       Normal|\n",
      "|       2|  National_Card|       Normal|\n",
      "+--------+---------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green.select('VendorID', givePaymentName(df_green.payment_type).alias('PaymentTypeName'), giveTripTypeName(df_green.trip_type).alias(\"TripTypeClass\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2505d3fb-f2cd-4ef8-b4e8-6ac54bc94b14",
   "metadata": {},
   "source": [
    "TODO: Is there an UDF that could not be transformed into a Pandas' UDF?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44eb6f-be6e-472b-abf5-fe417543bf85",
   "metadata": {},
   "source": [
    "Now, what if ,instead of passing a column to our UDF, we want to pass it two or three? We could just pass several `pd.Series` as parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f95f1be-62ff-44c0-b644-2ac7a78bf11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|VendorId|TipPercentage|\n",
      "+--------+-------------+\n",
      "|       2|         20.0|\n",
      "|       2|          0.0|\n",
      "|       2|          0.0|\n",
      "|       2|      16.6712|\n",
      "|       2|    16.666666|\n",
      "|       2|          0.0|\n",
      "|       2|    61.281338|\n",
      "|       2|          0.0|\n",
      "|       2|    16.666666|\n",
      "|       2|          0.0|\n",
      "|       2|    19.985518|\n",
      "|       2|     5.263158|\n",
      "|       1|          0.0|\n",
      "|       1|          0.0|\n",
      "|       2|    5.9620595|\n",
      "|       2|    16.666666|\n",
      "|       2|    16.666666|\n",
      "|       2|    16.666666|\n",
      "|       2|    6.6666665|\n",
      "|       2|          0.0|\n",
      "+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@F.pandas_udf(returnType=T.FloatType())\n",
    "def tip_percent_of_fare_1(tip_amount: pd.Series, total_amount: pd.Series) -> pd.Series:\n",
    "    return (tip_amount / total_amount) * 100\n",
    "\n",
    "display(df_green.select(F.col('VendorId'), tip_percent_of_fare_1(F.col('tip_amount'), F.col('total_amount')).alias('TipPercentage')).show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92526e4c-6bb2-4f29-81bd-dbca451f2cec",
   "metadata": {},
   "source": [
    "All previous UDFs are processed in batches. What if there is an operation we do not want to repeat over an over every batch? What if we only want to do it once and use the result of said operation every batch? (loading a ML model, for example). In this case, instead of defining the input and output of our UDF as a pd.Series, we could define it as an iterator of batches of pd.Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e503e0c0-6c94-4a16-b9e5-b5ef8e5c2f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-------------+\n",
      "|VendorID|PaymentTypeName|TripTypeClass|\n",
      "+--------+---------------+-------------+\n",
      "|       2|           Cash|       Normal|\n",
      "|       2|  National_Card|       Normal|\n",
      "|       2|  National_Card|      Premium|\n",
      "|       2|           Cash|       Normal|\n",
      "|       2|           Cash|       Normal|\n",
      "|       2|  National_Card|       Normal|\n",
      "|       2|           Cash|       Normal|\n",
      "|       2|  National_Card|       Normal|\n",
      "|       2|           Cash|       Normal|\n",
      "|       2|  National_Card|       Normal|\n",
      "+--------+---------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "@F.pandas_udf(T.StringType())\n",
    "def give_payment_name(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:   # We are overwritting the method from above\n",
    "    # Some very expensive operation\n",
    "    time.sleep(10)\n",
    "    \n",
    "    try:\n",
    "        for x in batch_iter:\n",
    "            yield x.map({\n",
    "                1: \"Cash\",\n",
    "                2: \"National_Card\",\n",
    "                3: \"International_Card\",\n",
    "                4: \"Bizum\",\n",
    "                5: \"Begged_for_mercy\"\n",
    "            }).fillna(\"Null?\")\n",
    "    finally:\n",
    "        pass  # release resources here, if anytest_df.select(square_plus_y(col(\"id\"))).show()\n",
    "\n",
    "df_green.select('VendorID', give_payment_name(df_green.payment_type).alias('PaymentTypeName'), giveTripTypeName(df_green.trip_type).alias(\"TripTypeClass\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb40700-4fe7-41e5-8e54-f5799c31e9fc",
   "metadata": {},
   "source": [
    "And what if we need to use this solution but use several columns as input? No problem either:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e1c0afe-4f53-4f3d-8936-f2b7087c447e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|VendorId|TipPercentage|\n",
      "+--------+-------------+\n",
      "|       2|         20.0|\n",
      "|       2|          0.0|\n",
      "|       2|          0.0|\n",
      "|       2|      16.6712|\n",
      "|       2|    16.666666|\n",
      "|       2|          0.0|\n",
      "|       2|    61.281338|\n",
      "|       2|          0.0|\n",
      "|       2|    16.666666|\n",
      "|       2|          0.0|\n",
      "|       2|    19.985518|\n",
      "|       2|     5.263158|\n",
      "|       1|          0.0|\n",
      "|       1|          0.0|\n",
      "|       2|    5.9620595|\n",
      "|       2|    16.666666|\n",
      "|       2|    16.666666|\n",
      "|       2|    16.666666|\n",
      "|       2|    6.6666665|\n",
      "|       2|          0.0|\n",
      "+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@F.pandas_udf(returnType=T.FloatType())\n",
    "def tip_percent_of_fare(values: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
    "    # Some very expensive operation\n",
    "    time.sleep(10)\n",
    "\n",
    "    for tip, total in values:\n",
    "        yield (tip / total) * 100\n",
    "\n",
    "display(df_green.select(F.col('VendorId'), tip_percent_of_fare(F.col('tip_amount'), F.col('total_amount')).alias('TipPercentage')).show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1175251-62ab-4618-8398-6fb3d0ddc676",
   "metadata": {},
   "source": [
    "All the operations above return a series of the same lenght of the input. What if we want something different? We can also do that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8a095e-9292-4d2d-915e-110eb9d69dec",
   "metadata": {},
   "source": [
    "We can return only one value per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d96a768d-2e9d-4ac0-8716-813d4da6a499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|VendorID|               var|\n",
      "+--------+------------------+\n",
      "|       2|15.678956050473168|\n",
      "|       1|13.682413066267353|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@F.pandas_udf(T.DoubleType())\n",
    "def GetStdDeviation(series: pd.Series) -> float:\n",
    "    return series.std()\n",
    "\n",
    "(\n",
    "    df_green.groupBy('VendorID')\n",
    "    .agg(\n",
    "        GetStdDeviation(df_green.total_amount).alias('var')\n",
    "    )\n",
    "    .orderBy('var', ascending=False)\n",
    "    .show(10)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e12c5e-743c-49e9-9487-12543dbad3a7",
   "metadata": {},
   "source": [
    "Or we can return a completely new struck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c6e83723-a0f3-464a-9fcf-fc6d06a2a7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|VendorID|      var|\n",
      "+--------+---------+\n",
      "|       1|13.682413|\n",
      "|       2|15.678956|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getVar(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.DataFrame({\n",
    "        'VendorID': [data['VendorID'].iloc[0]],  # Because of the previous groupBy(), each partition will only have one VendorID\n",
    "        'var': [data['total_amount'].std()]\n",
    "    })\n",
    "\n",
    "# We can also use the SQL version of schema: \n",
    "# schema = 'type string, amount double, amountNorm double'\n",
    "schema = T.StructType([\n",
    "    T.StructField('VendorID', T.IntegerType()),\n",
    "    T.StructField('var', T.FloatType())\n",
    "])\n",
    "\n",
    "(\n",
    "    df_green.groupBy('VendorID')\n",
    "    .applyInPandas(getVar, schema)   # When using this function, we do not need to define the method as an UDF.\n",
    "                                                # IMPORTANT! Apparently, this is now recommended in Spark 3 and the older method deprecated! \n",
    "                                                # But it can only be used when dealing with pd.DataFrame as inputs and outputs\n",
    "    .show(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d401903a-5527-4b7e-803c-5e60c870d33a",
   "metadata": {},
   "source": [
    "Assuming this last case (the number of records of the output does not need to be the same as the number of records of the input), the function you need to use depends on the output:\n",
    "\n",
    "| Input | Output | Function |\n",
    "| --- | --- | --- |\n",
    "| One row | One row | Pandas UDF |\n",
    "| Many rows | One | `applyInPandas()` or `groupBy().agg()`|\n",
    "| One row | Many rows | `mapInPandas()` or `explode`|\n",
    "* In this table I understand \"row\" as \"record\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8458129-fb2e-43bc-b3bb-9a5201e88079",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce11c80-a150-4080-a87c-a9c574eefbce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028f4005-6f15-426f-95e3-15fe73e0e7d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c18dfb2-587b-469a-995d-0592c919d571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05f44bb-9867-48ab-8e9e-85dfe8221ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e406c081-31e3-4718-81fc-e9ebc27faa83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6755794e-8453-46c6-bee9-bc3dc42fd504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
